#!/usr/bin/env python 
import web, json, sys, exceptions, re, datetime, urllib, select, psycopg2, ast, ee, uuid, math, cPickle, pdfkit, cgi, logging
from amazon_ses import AmazonSES, EmailMessage, AmazonError
from ee import EEException
from twilio.rest import TwilioRestClient
from decimal import Decimal
from psycopg2 import ProgrammingError, DataError, IntegrityError, extensions, OperationalError
from dbconnect import dbconnect, twilio, amazon_ses, google_earth_engine
from orderedDict import OrderedDict
from lxml import etree
from pyproj import Proj
from pyproj import transform

urls = (
  "/help", "help",
  "/terms", "terms",
  "/manager", "manager",
  "/gee/getDatesForBB", "getDatesForBB",
  "/gee/getDatesForPoint", "getDatesForPoint",
  "/gee/WMSServer", "geeWMS",
  "/gee/(.+)", "gee",
  "/(.+)/(.+)", "getservice",
  "/(.+)/", "getservices",
  "/", "getschemas"
  )

class DopaServicesError(Exception):
    """Exception Class that allows the DOPA Services REST Server to raise custom exceptions"""
    pass  

class CustomJSONEncoder(json.JSONEncoder):
    """Class to provide the correct serialisation of decimal values into JSON"""
    def default(self, obj):
        if isinstance(obj, Decimal):
            return float(obj)
        elif isinstance(obj, datetime.date):
            return obj.isoformat()
        return json.JSONEncoder.default(self, obj)

class getschemas:
    def GET(self):
        try:
            render = web.template.render('templates/')
            conn = dbconnect("species_appuser")
            conn.cur.callproc("utils.dopa_rest_getschemas")
            schemas = conn.cur.fetchall()
            schemasdict = [dict([('name', schema[0]), ('description', schema[1])]) for schema in schemas]
            return render.getschemas(schemasdict, render.header(), render.footer())
        
        except (DopaServicesError, ProgrammingError, OperationalError):
            return "DOPA Services Error: " + str(sys.exc_info())

class getservices:
    def GET(self, schemaname):
        try:
            render = web.template.render('templates/')
            conn = dbconnect("species_appuser")
            conn.cur.callproc("utils.dopa_rest_getservices", [schemaname])
            services = conn.cur.fetchall()
            servicesdict = [dict([('name', service[0]), ('description', getservicedescription(service[1]))]) for service in services if isValidServiceName(service[0])]
            return render.getservices(schemaname, servicesdict, render.header(), render.footer())

        except (DopaServicesError, ProgrammingError, OperationalError):
            return "DOPA Services Error: " + str(sys.exc_info())

class getservice:
    def GET(self, schemaname, servicename):
        # if there are some parameters then the user is calling the service
        if web.input():
            return callservice(schemaname, servicename, web.ctx.query[1:])  # pass the querystring to preserve the order of the parameters - the web.input() collection does not preserve the order
        else:
            try:
                render = web.template.render('templates/')
                conn = dbconnect("species_appuser")
                conn.cur.callproc("utils.dopa_rest_getservice", [servicename])
                params = conn.cur.fetchall()
                if (len(params) == 0):
                    raise DopaServicesError('No parameters found for service ' + servicename)
                
                # parse the description text to get the parameters descriptions - the parameter descriptions are encoded using {<param_desc>$<param_desc>$<param_desc> etc}
                paramdesc = []
                paramdescgroups = re.search('{.*}', params[0][1].replace("\n", ""))  # replace line feeds otherwise the regex doesnt work
                if (paramdescgroups):
                    paramdesc = paramdescgroups.group(0)[1:-1].split("$")
                # fill in the parameter descriptions if they have not been written
                paramdesc[len(paramdesc):] = ['No description' for i in range(len(params) - len(paramdesc))] 
                
                # parse the function definition for default parameter values
                paramdefs = []
                paramdefsstr = params[0][5]
                if 'DEFAULT ' in paramdefsstr:
                    # get the position of the parameter names in the parameter definition string
                    pos = [paramdefsstr.find(param[3] + ' ') for param in params if (param[2] == 'IN')]
                    # add on the length of the parameter definition to get the last parameter definition
                    pos.append(len(paramdefsstr))
                    # get the parameter definitions as a list
                    paramdefs = [paramdefsstr[pos[i]:pos[i + 1]] for i in range(len(pos) - 1)]
                    # remove any trailing spaces with commas
                    paramdefs = [(p[:-2] if p[-2:] == ', ' else p) for p in paramdefs]
                    # remove the DEFAULT statement
                    paramdefs = [(p[p.find('DEFAULT') + 8:] if 'DEFAULT' in p else '') for p in paramdefs]
                    # remove the  ARRAY[] statement
                    paramdefs = [(p[6:-1] if 'ARRAY' in p else p) for p in paramdefs]
                    # remove any typecast symbols, e.g. ::text
#                    paramdefs = [p[:p.find('::')] if '::' in p else p for p in paramdefs] # some are complicated, e.g. ['wdpa_id integer, ', "rlstatus character[] DEFAULT ARRAY['EN'::text, 'CR'::text, 'VU'::text, 'NT'::text, 'LC'::text, 'EX'::text, 'EW'::text, 'DD'::text]"]
                    paramdefs = [p.replace("::text", "") for p in paramdefs]
                    paramdefs = [p.replace("::integer", "") for p in paramdefs]
                    paramdefs = [p.replace("::character varying", "") for p in paramdefs]
                    # remove any quotes, e.g. 'CR','DD' -> CR, DD
                    paramdefs = [p.replace("'", "") for p in paramdefs]
                    # remove any spaces, e.g. CR, DD -> CR,DD
                    paramdefs = [p.replace(" ", "") for p in paramdefs]
                # fill in the paramdefs
                paramdefs[len(paramdefs):] = ['' for i in range(len(params) - len(paramdefs))]
#                return params
                # create a dictionary containing the parameter information
                paramsdict = [dict([('mode', params[i][2]), ('name', params[i][3]), ('type', gettypefrompostgresql(params[i][4])), ('description', paramdesc[i]), ('default', paramdefs[i])]) for i in range(len(params))]
                return render.getservice(schemaname, servicename, getservicedescription(params[0][1]), [p for p in paramsdict if (p['mode'] == 'IN')], [p for p in paramsdict if (p['mode'] == 'OUT')], render.header(), render.footer())
            
            except (DopaServicesError, ProgrammingError, OperationalError):
                return "DOPA Services Error: " + str(sys.exc_info())

def callservice(schemaname, servicename, querystring):
    try:
        t1 = datetime.datetime.now()
        # log the request
        logging.basicConfig(filename='../../htdocs/mstmp/REST_Services_Log.log', level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s',)
#         logging.info("REST REQUEST: " + web.ctx.home + web.ctx.path + web.ctx.query)
        # PARSE THE STANDARD OPTIONAL INPUT PARAMETERS
        # get the input parameters
        params = OrderedDict([(q.split("=")[0], urllib.unquote(q.split("=")[1])) for q in querystring.split("&")])  # the unquoting is to handle encoded parameters (like from extJS - 1,2,3 as a parameter becomes 1%2C2%2C3
        # get the standard optional parameters from the url 
        format = params.setdefault('format', 'json') 
        fields = params.setdefault('fields', '').split(",")  # fields will be passed as an array, e.g. iucn_species_id,wdpa_id
        callback = params.setdefault('callback', None)
        includemetadata = params.setdefault('includemetadata', 'true')
        metadataName = params.setdefault('metadataname', 'metadata')
        rootName = params.setdefault('rootname', 'records')
        parseparams = params.setdefault('parseparams', 'true')
        isHadoop = ('true' if (servicename[-2:] == '_h') else 'false')  # if the service is a call to a hadoop method then set a flag 

        # remove the standard optional parameters from the dictionary so we are left with just the parameters required for the function
        del (params['format'], params['fields'], params['callback'], params['includemetadata'], params['parseparams'], params['metadataname'], params['rootname'])

        # connect to the database to get the data
        conn = dbconnect("species_especies_schema")

        # if it is a Hadoop query then we need to run if first before we actually use the values to get the data from postgresql 
        if (isHadoop.lower() == 'true'): 
            hadoopData = runHadoopQuery(conn, servicename, params)
            if hadoopData == '[]': hadoopData = '[-1]'
            servicename = "_" + servicename  # now call the postgresql function
            params.clear()
            params['species_ids'] = str(hadoopData)[1:-1];

        # PARSE AND CONVERT THE DATA TYPES OF THE OTHER INPUT PARAMETERS
        # get the parameters for the function from postgresql
        conn.cur.callproc('utils.dopa_rest_getparams', [servicename])
        # get the function parameters as a string and split this into a list, e.g. wdpa_id integer, presence_id integer[] -->  ['wdpa_id integer', ' presence_id integer[]']
        functionparams = conn.cur.fetchone()
        hasparams = True if functionparams[0] else False
        if hasparams:
            functionparams = functionparams[0].split(',')  
            # get the names of the function parameters which are array types
            arrayparamnames = [p.strip().split(" ")[0] for p in functionparams if '[' in p]
            # convert the array values into lists
            for key in params.keys():
                if key in arrayparamnames:
                    strlist = params[key].split(",")
                    isnum = isNumeric(strlist[0])
                    if isnum:
                        params[key] = [int(s) for s in strlist]
                    else:
                        params[key] = strlist
            # get the full list of function parameter names
            functionparamnames = [p.strip().split(" ")[0] for p in functionparams]
            # check that all parameters are correct
            invalidparamnames = [n for n in params.keys() if n not in functionparamnames]
            if invalidparamnames and parseparams == 'true':
                raise DopaServicesError('Invalid parameters: ' + ",".join(invalidparamnames))
            # put the input parameters in the right order 
            params = OrderedDict([(n, params[n]) for n in functionparamnames if n in params.keys()])
        
        # CHECK THE SERVICE NAME IS VALID
        if not (isValidServiceName(servicename)):
            raise DopaServicesError('Invalid servicename')
        
        # AUTHORISE ACCESS IF THE SERVICE CALL REQUIRES IT
        if requiresAuthentication(servicename):
            vals = casAuthenticate(servicename)
            if not(vals):
                raise DopaServicesError('Unauthorised access')
            else:
                return vals    

        # RUN THE QUERY
        if hasparams :
            if len(functionparams) != len(params):  # not all parameters are being passed to the function - so we need to use named parameters
                sql = "SELECT * from " + schemaname + "." + servicename + "(" + ",".join([n + ":=%(" + n + ")s" for n in params]) + ");"  # run the query using named parameters
    #            return 'sql: ' + sql + ' params: ' + str(params) + ' query: ' + conn.cur.mogrify(sql, params)
                conn.cur.execute(sql, params)
            else:
                conn.cur.callproc(schemaname + "." + servicename, params.values())  # run the query using positional parameters
        else:
            conn.cur.callproc(schemaname + "." + servicename)  # run the query using positional parameters
        rows = conn.cur.fetchall()

        # PROCESS THE ROWS AND WRITE THEM BACK TO THE CLIENT
        conn.cur.close()
        t2 = datetime.datetime.now()
        
        # METADATA SECTION OF RESPONSE
        allfields = [d.name for d in conn.cur.description]
        fieldcount = len(allfields)
        if (fields == ['']): fields = allfields 
#        return conn.cur.description
        fieldsdict = [dict([("name", d.name), ("type", gettypefromtypecode(d.type_code))]) for d in conn.cur.description if (d.name in fields)]
        if len(fieldsdict) != len(fields):
            raise DopaServicesError('Invalid output fields')
        metadatadict = OrderedDict([("duration", str(t2 - t1)), ("error", None), ("idProperty", conn.cur.description[0].name), ("successProperty", 'success'), ("totalProperty", 'recordCount'), ("success", True), ("recordCount", int(conn.cur.rowcount)), ("root", rootName), ("fields", fieldsdict)])    
        
        # RECORDS SECTION OF THE RESPONSE
        colsRequired = [allfields.index(field) for field in fields]
        if format in ['json', 'array']:
            if format == 'json':
                recordsdict = [OrderedDict([(allfields[col], row[col]) for col in range(fieldcount) if (col in colsRequired)]) for row in rows] 
            else:
                recordsdict = [[row[col] for col in range(fieldcount) if (col in colsRequired)] for row in rows]
            json.encoder.FLOAT_REPR = lambda f: ("%.4f" % f)  # this specifies how many decimal places are returned in the json with float values - currently set to 4
#             return rows,recordsdict
            if (includemetadata.lower() == 'true'):
                responsejson = json.dumps(dict([(metadataName, metadatadict), (rootName, recordsdict)]), indent=1, cls=CustomJSONEncoder)
            else: 
                responsejson = json.dumps(dict([(rootName, recordsdict)]), indent=1, cls=CustomJSONEncoder)
            if callback:
                web.header("Content-Type", "application/javascript") 
                return callback + '(' + responsejson + ');'                 
            else:
                web.header("Content-Type", "application/json") 
                return responsejson
        
        elif format == 'xml':
            root = etree.Element('results')
        #            THERE ARE ISSUES WITH THE XML SERIALISER PARSING INTEGERS AND IN THE FIELD TYPES THERE ARE INTEGERS SO THE METADATADICT OBJECT CAUSES THE XML PARSER TO FAIL AT THE MOMENT SO LEAVING OUT METADATA
        #            if (includemetadata.lower() == 'true'):
        #                return metadatadict
        #                metadatanode = xml.Element('metadata', metadatadict)
        #                root.append(metadatanode)
        #                xmlfieldsdicts = [dict([("name", d.name), ("type", str(d.type_code))]) for d in conn.cur.description if (d.name in fields)] # element tree cannot serialise integers so we have to convert these to strings
        #                fieldselements = [xml.Element('field', dictionary) for dictionary in xmlfieldsdicts]
        #                fieldsnode = xml.Element('fields')
        #                for fieldelement in fieldselements:
        #                    fieldsnode.append(fieldelement)
        #                metadatanode.append(fieldsnode)
            recordsdicts = [OrderedDict([(allfields[col], str(row[col]).decode('utf-8')) for col in range(fieldcount) if (col in colsRequired) and str(row[col]) != 'None']) for row in rows ]  #
            recordselements = [etree.Element('record', element) for element in recordsdicts]
            recordsnode = etree.Element(rootName)
            for recordelement in recordselements:
                recordsnode.append(recordelement)
            root.append(recordsnode)
            web.header("Content-Type", "text/xml")
#             web.header("Content-Type", "application/Excel") # doesnt work!
#             web.header("Content-Disposition", "attachment; filename=test.xml")
            return etree.tostring(root)
        
        elif format == 'sms':
            _twilio = twilio()
            client = TwilioRestClient(_twilio.twilio_account_sid, _twilio.twilio_auth_token)  # use the twilio api account
            bodystr = 'Hi Andrew - test species data: '
            bodystr = bodystr + str(rows[0])[:160 - len(bodystr)]
            message = client.sms.messages.create(to="+393668084920", from_="+19712647662", body=bodystr)  # my mobile
            return message

        elif format == 'email':
            _amazon_ses = amazon_ses()
            amazonSes = AmazonSES(_amazon_ses.AccessKeyID, _amazon_ses.SecretAccessKey)  # use the amazon simple email service api account
            message = EmailMessage()
            message.subject = 'DOPA Information Request'
            message.bodyHtml = getResultsAsHTML(rows, fieldcount, colsRequired, metadatadict) 
            result = amazonSes.sendEmail('a.cottam@gmail.com', 'a.cottam@gmail.com', message)  # to me
            return result 
                    
        elif format == 'html':
            htmlData = getResultsAsHTML(rows, fieldcount, colsRequired, metadatadict) 
            web.header("Content-Type", "text/html") 
            return "<html><head></head><body>" + htmlData + "</body></html>"
        
        elif format == 'csv':
#             web.header("Content-Type", "text/csv") # downloads a file 
            data = [[row[col] for col in range(fieldcount) if (col in colsRequired)] for row in rows]
            colnames = ",".join([f["name"] for f in metadatadict["fields"]]) + "\n"
            output = colnames + "\n".join([p for p in [",".join(h) for h in [[getStringValue(col) for col in row] for row in data]]]) 
            return output

        elif format == 'pdf':    
            config = pdfkit.configuration(wkhtmltopdf='/usr/local/bin/wkhtmltopdf')
            web.header("Content-Type", "application/pdf")
            htmlData = getResultsAsHTML(rows, fieldcount, colsRequired, metadatadict)
            return pdfkit.from_string(htmlData.decode('utf8'), False, configuration=config, options={'quiet': '', 'encoding': "UTF-8"})
        
        else:
            raise DopaServicesError('Invalid response format: ' + format)

    except (DopaServicesError, DataError, ProgrammingError, exceptions.TypeError, IndexError, IntegrityError, AmazonError, OperationalError) as e:
#        web.webapi.internalerror() #returns a internal server error 500
        t2 = datetime.datetime.now()
        msg = "There was an error sending the email. Make sure that the email address has been verified in Amazon Simple Email Services" if type(e) == AmazonError else str(sys.exc_info()).decode('string_escape')
        logging.error(msg + "\n")
        if format in ['json', 'array']:
            metadatadict = OrderedDict([("duration", str(t2 - t1)), ("error", msg), ("idProperty", None), ("successProperty", 'success'), ("totalProperty", 'recordCount'), ("success", False), ("recordCount", 0), ("root", None), ("fields", None)])    
            responsejson = json.dumps(dict([(metadataName, metadatadict), (rootName, None)]), indent=1)
            if callback:
                web.header("Content-Type", "application/javascript") 
                return callback + '(' + responsejson + ');'                 
            else:
                web.header("Content-Type", "application/json") 
                return responsejson

        else:
            return "DOPA Services Error: " + msg
            
def getStringValue(value):
    if value is not None:
        return str(value)
    else:
        return ''

def getResultsAsHTML(rows, fieldcount, colsRequired, metadatadict, landscape=False):  # set landscape to True to set orientation to landscape
    data = [[row[col] for col in range(fieldcount) if (col in colsRequired)] for row in rows]
    colnames = "<tr>" + "".join(["<th>" + f["name"] + "</th>" for f in metadatadict["fields"]]) + "</tr>"
    html = "<table>" + colnames + "".join(["<tr>" + p + "</tr>" for p in ["".join(h) for h in [['<td>' + getStringValue(col) + '</td>' for col in row] for row in data]]]) + "</table>" 
    if landscape:
        return "<head><meta name='pdfkit-orientation' content='Landscape'/></head>" + html + "</table>"
    else:
        return html
            
def gettypefromtypecode(typecode):  # returns a string representation of the psycopg2.cursor.type_code value which is shown in the response - the values come from the pg_type table in PostGIS and these are not complete yet - in the output parameter data type section
    if (typecode in [16, 1000, 1560, 1561]): return "boolean"
    elif (typecode in [20, 21, 23]): return "integer"
    elif (typecode in [26, 790, 1005, 1007, 1028, 1016, 1021, 1022, 1700]): return "number"
    elif (typecode in [700, 701]): return "float"
    elif (typecode in [18, 25, 1043, 1002, 1009, 1015, 1043]): return "string"
    elif (typecode in [702, 703, 704, 1023, 1024, 1025, 1082, 1083, 1084, 1182, 1183, 1184, ]): return "date"
    elif (typecode in [17, 22, 24, 27, 28, 29, 30]): return "object"
    elif (typecode in [2278]): return "Null"
    else: return "Undefined"
    
def gettypefrompostgresql(postgresqltype):  # returns a string representation of the SQL data type - this is used to show the data type in the html pages
    if (postgresqltype.lower() in ['integer', 'bigint']): return "integer"
    elif (postgresqltype.lower() in ['boolean']): return "boolean"
    elif (postgresqltype.lower() in ['single precision']): return "single"
    elif (postgresqltype.lower() in ['double precision']): return "double"
    elif (postgresqltype.lower() in ['numeric']): return "numeric"
    elif (postgresqltype.lower() in ['array']): return "array"
    elif (postgresqltype.lower() in ['character varying', 'text']): return "string"
    elif (postgresqltype.lower() in ['date']): return "date"
    elif (postgresqltype.lower() in ['timestamp with time zone']): return "datetime"
    else: return "unknown"

def getservicedescription(fulldescription):
    pos = fulldescription.find("{")
    if pos > -1:
        return fulldescription[:fulldescription.find("{")]
    else:
        return fulldescription

def isNumeric(val):
    try:
        i = float(val)
    except ValueError, TypeError:
        return False
    else:
        return True

def isValidServiceName(servicename):
    if (servicename[:3] in ['get', 'set']) | ((servicename[:1] == '_') & (web.ctx.host == 'dopa-services.jrc.it')):
        return True
    else:
        return False

def requiresAuthentication(servicename):
    if (servicename[:3] in ['set']) | ((servicename[:4] == '_set') & (web.ctx.host == 'dopa-services.jrc.it')):
        return True
    else:
        return False

def casAuthenticate(servicename):
    CAS_SERVER = "https://intragate.ec.europa.eu"
    if "&parseparams=false" not in web.ctx.query:
        THIS_SCRIPT = urllib.quote(web.ctx.home + web.ctx.path + web.ctx.query + "&parseparams=false")
    else: 
        THIS_SCRIPT = urllib.quote(web.ctx.home + web.ctx.path + web.ctx.query)
#     logging.info("Authenticating to CAS using " + THIS_SCRIPT)    
    status, id, cookie = login(CAS_SERVER, THIS_SCRIPT)
#     logging.info("Login completed. status: " + str(status) + " id: " + str(id) + " cookie: " + cookie + "\n")      
    return (status, id, cookie)

def runHadoopQuery(conn, functionname, params): 
    try:
        conn.cur.execute("INSERT INTO hadoop_jobs(id, functionname, params) VALUES (DEFAULT,'" + functionname + "','" + params['quadkey'] + "') RETURNING id")
        job_id = conn.cur.fetchone()[0]  # this is the job_id value from the table
        conn.cur.execute("LISTEN hadoop_job_complete;")  # listen for the results to be posted back
        conn.cur.execute("NOTIFY hadoop_job_request,'" + str(job_id) + "," + functionname + "," + params['quadkey'] + "'")
        while 1:
            if select.select([conn.conn], [], [], 5) == ([], [], []):
                assert 'nothing'
            else:
                conn.conn.poll()
                while conn.conn.notifies:  # results posted back
                    conn.cur.execute("UNLISTEN hadoop_job_complete;")
                    resultsDict = ast.literal_eval(conn.conn.notifies.pop().payload)
                    return resultsDict['results']
    except (DopaServicesError, OperationalError):
        return "DOPA Services Error: " + str(sys.exc_info())

class manager:
    def GET(self):
        try:
            render = web.template.render('templates/')
            conn = dbconnect("species_appuser")
            conn.cur.execute("select * from especies.species_wdpa_log")
            rows = conn.cur.fetchall()
            return render.manager(rows, render.header_manager(), render.footer_manager())
        
        except (DopaServicesError, ProgrammingError, OperationalError):
            return "DOPA Services Error: " + str(sys.exc_info())

class help:
    def GET(self):
        render = web.template.render('templates/')
        return render.help(render.header(), render.footer())

class terms:
    def GET(self):
        render = web.template.render('templates/')
        return render.terms(render.header(), render.footer())

class gee:
    def GET(self, id):
        try:
            gee_authenticate_stored()
            landsat_scene = ee.Image(id)
            landsatlook_thumbnail = landsat_scene.getThumbUrl({'bands': 'B6,B5,B4', 'size': '600'})
#             web.header("Content-Type", "images/png")        #returns as a file
            return urllib.urlopen(landsatlook_thumbnail)  # streams the image back to the client
        except (DopaServicesError, ProgrammingError, EEException):
            return "DOPA Services Error: " + str(sys.exc_info())        

class getDatesForBB:
    def GET(self):
        # request will be in the format getDatesForBB?BBOX=-24867967,734592,12389274,8561744&CRS=EPSG:102100
        ll_x, ll_y, ur_x, ur_y = web.input()['BBOX'].split(",")
        bbox_latlong = getBoundingBoxLL(ll_x, ll_y, ur_x, ur_y, web.input()['CRS'])
        gee_authenticate_stored()
        clip_polygon = ee.Feature.Polygon([[[115.23007047049605, 6.832121400646544], [114.86071749550347, 5.091937003312537], [116.53236986665138, 4.730846019175226], [116.90729867328733, 6.473882787770036], [115.23007047049605, 6.832121400646544]]])
        return clip_polygon["coordinates"], ee.Feature.Polygon([bbox_latlong])["coordinates"]
        return ee.ImageCollection("LANDSAT/LC8_L1T").filterBounds(clip_polygon).aggregate_array("DATE_ACQUIRED").getInfo()

class getDatesForPoint:
    def GET(self):
        # request will be in the format getDatesForPoint?POINT=-24867967,734592&CRS=EPSG:102100
        x, y = web.input()['POINT'].split(",")
        lng, lat = getPointLL(x, y, web.input()['CRS'])
        # points dont work with GEE, so we have to create a small polygon
        small_query_polygon = [[[lng - 0.1, lat + 0.1], [lng - 0.1, lat - 0.1], [lng + 0.1, lat - 0.1], [lng + 0.1, lat + 0.1], [lng - 0.1, lat + 0.1]]]
#         return web.input()['callback'] + "({'wibble':1})"
        gee_authenticate_stored()  # authenticate to GEE 
        stringDates = ee.ImageCollection("LANDSAT/LC8_L1T").filterBounds(ee.Feature.Polygon(small_query_polygon)).aggregate_array("DATE_ACQUIRED").getInfo()  # get the dates as strings
        dates = [dateToDateTime(s) for s in stringDates]
        dates = [s.isoformat() for s in sorted(set(dates))]  # convert them to properly formatted strings 
        web.header("Content-Type", "text/javascript")
        return web.input()['callback'] + "({'dates':['" + "','".join([d for d in dates]) + "']})"  # return as a jsonp

class geeWMS:
    def GET(self):
        try:
            gee_authenticate_stored()  # authenticate using stored credentials and API definitions
#             gee_authenticate()
#             return self.getTestImage()
            self.parseWMSParams(web.input()) 
            if self.version == "1.3.0":  # get the ll and ur x/y coordinates
                ll_x, ll_y, ur_x, ur_y = self.bbox.split(",")
            else:
                ll_x, ll_y, ur_x, ur_y = self.bbox.split(",")  # same for the moment            
            self.bounding_box = getBoundingBoxLL(ll_x, ll_y, ur_x, ur_y, self.crs)  # get the bounding box as lat/long suitable for sending to GEE API
            d, m, y = [int(s) for s in self.date.split("-")]
            collection = ee.ImageCollection(self.dataset).filterBounds(ee.Feature.Polygon([self.bounding_box])).filterDate(datetime.datetime(y, m, d - 1), datetime.datetime(y, m, d + 1))
            
            # cloud correction
            if self.remove_cloud:
                image = self.cloudCorrection(collection)
            else:
                image = collection.median()
            
            # illumination correction
            if self.remove_illumination:
                image = self.illuminationCorrection(image)
                
            # colour and contrast adjustment
            if (self.bands == "B4,B3,B2") or (self.bands == "B2,B3,B4"): 
                image = ee.Image.cat([image.expression("b('B4')+770"), image.select('B3'), image.expression("b('B2')-1230")]);
            min = 7000
            max = 10000
            
            # create the thumbnail
            collection_image_thumbnail = image.getThumbUrl({'bands': self.bands, 'size': self.width, 'min': min, 'max': max, 'region': self.bounding_box})
            
#             web.header("Content-Type", "images/png")                #returns as a file
            return urllib.urlopen(collection_image_thumbnail)  # streams the image back to the client
        
        except (DopaServicesError, ProgrammingError, EEException):
            return "DOPA Services Error: " + str(sys.exc_info())        
        
    def getTestImage(self):
        image = ee.Image('LC81170572013170LGN00')
        image_thumbnail = image.getThumbUrl({'bands': 'B6,B5,B4', 'size': '600'})
        return urllib.urlopen(image_thumbnail)  # streams the image back to the client
               
    def parseWMSParams(self, parameters):
        # example parameters {'STYLES': u'', 'LAYERS': u'1', 'SERVICE': u'WMS', 'CRS': u'EPSG:102100', 'FORMAT': u'image/png', 'REQUEST': u'GetMap', 'HEIGHT': u'400', 'WIDTH': u'1904', 'VERSION': u'1.3.0', 'BBOX': u'-24867967,734592,12389274,8561744', 'TRANSPARENT': u'TRUE'}>
        # get the gee params
        self.style = parameters['STYLES']
        gee_params = parameters['LAYERS'].split("!")
        self.dataset = gee_params[0]
        self.date = gee_params[1]
        self.remove_cloud = (gee_params[2] == "1")
        self.remove_illumination = (gee_params[3] == "1")
        self.bands = gee_params[4]
        
        self.service = parameters['SERVICE']
        self.crs = parameters['CRS']
        self.format = parameters['FORMAT']
        self.request = parameters['REQUEST']
        self.height = parameters['HEIGHT']
        self.width = parameters['WIDTH']
        self.version = parameters['VERSION']
        self.bbox = parameters['BBOX']
        self.transparent = parameters['TRANSPARENT']
        
    def illuminationCorrection(self, image):
        # accepts either a single scene which will have the sun elevation and azimuth already populated, or a collection image which will need to have the sun elevation and azimuth manually populated and accessed via the properties
        try:
            terrain = ee.call('Terrain', ee.Image('srtm90_v4'))
            # solar_zenith = (90 - image.getInfo()['properties']['SUN_ELEVATION'])
            solar_zenith = 31.627850000000002
            # solar_azimuth = image.getInfo()['properties']['SUN_AZIMUTH']
            solar_azimuth = 50.377735
            solar_zenith_radians = (solar_zenith * math.pi) / 180
            slope_radians = terrain.select(['slope']).expression("(b('slope')*" + str(math.pi) + ")/180")
            aspect = terrain.select(['aspect'])
            cosZ = math.cos(solar_zenith_radians)
            cosS = slope_radians.cos()
            slope_illumination = cosS.expression("b('slope')*(" + str(cosZ) + ")").select(['slope'], ['b1'])
            sinZ = math.sin(solar_zenith_radians)
            sinS = slope_radians.sin()
            azimuth_diff_radians = aspect.expression("((b('aspect')-" + str(solar_azimuth) + ")*" + str(math.pi) + ")/180")
            cosPhi = azimuth_diff_radians.cos()
            aspect_illumination = cosPhi.multiply(sinS).expression("b('aspect')*" + str(sinZ)).select(['aspect'], ['b1'])
            ic = slope_illumination.add(aspect_illumination)
            return image.expression("((image * (cosZ + coeff)) / (ic + coeff)) + offsets", {'image': image.select('B4', 'B3', 'B2'), 'ic': ic, 'cosZ': cosZ, 'coeff': [12, 9, 25], 'offsets': [0, 0, 0]})
        except (DopaServicesError, ProgrammingError, EEException):
            return "DOPA Services Error: " + str(sys.exc_info())        

    def cloudCorrection(self, collection):
        cloudless_images = []
        lsb_val = int(math.pow(2, 14))
        msb_val = int(math.pow(2, 15))
        for feature in collection.getInfo()['features']:
            image = ee.Image(feature['id'])
            cloud = image.expression("(2*(b('BQA')&" + str(msb_val) + ")/" + str(msb_val) + ")+((b('BQA')&" + str(lsb_val) + ")/" + str(lsb_val) + ")")
            cloud_mask = cloud.expression("(b('constant')==1)")
            masked = image.mask(cloud_mask)
            output = masked.toUint16().focal_median()
            cloudless_images.append(output)
        return ee.ImageCollection(cloudless_images).median()
    
def dateToDateTime(_date):  # converts a Google date into a Python datetime, e.g. 2013-06-14 to datetime.datetime(2013, 5, 2, 0, 0))
    d = _date.split("-")
    return datetime.datetime(int(d[0]), int(d[1]), int(d[2]))
    
def createGoogleAPIJson():
    gee_authenticate()
    f = open(r'google earth engine/_api.pickle', 'wb')
    cPickle.dump(ee.ApiFunction._api, f)
    f.close()

def gee_authenticate():
    _google_earth_engine = google_earth_engine()  # authenticate directly with GEE API
    ee.Initialize(ee.ServiceAccountCredentials(_google_earth_engine.MY_SERVICE_ACCOUNT, _google_earth_engine.MY_PRIVATE_KEY_FILE)) 

def gee_authenticate_stored():
    # initialise the authentication with Google Earth Engine
    f_credentials = open(r'google earth engine/_credentials.pickle', 'rb')
    ee.data.initialize(cPickle.load(f_credentials))
    f_credentials.close()
    
    # initialise the API
    f_api = open(r'google earth engine/_api.pickle', 'rb')  # pickle of the gee api functions in json format
    ee.ApiFunction._api = cPickle.load(f_api)
    f_api.close()
    ee.Image.initialize()
    ee.Feature.initialize()
    ee.Collection.initialize()
    ee.ImageCollection.initialize()
    ee.FeatureCollection.initialize()
    ee.Filter.initialize()
    ee.Geometry.initialize()
    ee.String.initialize()
    ee._InitializeGeneratedClasses()
    ee._InitializeUnboundMethods()

def getBoundingBoxLL(ll_x, ll_y, ur_x, ur_y, crs):  # gets a lat/long bounding box suitable for sending to the Google Earth Engine API
        ll_long, ll_lat = getPointLL(ll_x, ll_y, crs)  # transform the data to lat/long so that we can use it in Google Earth Engine
        ur_long, ur_lat = getPointLL(ur_x, ur_y, crs)
        #             return "ll_x: " + str(ll_x) + " ll_y: " +  str(ll_y) + " ur_x: " + str(ur_x) + " ur_y: " + str(ur_y) + "\nll_long: " + str(ll_long) + " ll_lat: " + str(ll_lat) + " ur_long: " + str(ur_long) + " ur_lat: " + str(ur_lat) + "\nregion: " + str(region)
        return [[ll_long, ur_lat], [ll_long, ll_lat], [ur_long, ll_lat], [ur_long, ur_lat]]  # get the area of interest

def getPointLL(point_x, point_y, crs):
        p1, p2 = getProjections(crs)
        ll_long, ll_lat = transform(p1, p2, point_x, point_y)  # transform the data to lat/long so that we can use it in Google Earth Engine
        return ll_long, ll_lat

def getProjections(crs):
        if crs.upper() == "EPSG:102100":
            p1 = Proj(init='epsg:3857')
        else:
            p1 = Proj(init=crs)
        p2 = Proj(init='epsg:4326')
        return p1, p2    
    
SECRET = "7e16162998eb7efafb1498f75190a937"

#  Name field for pycas cookie
PYCAS_NAME = "pycas"

#  CAS Staus Codes:  returned to calling program by login() function.
CAS_OK = 0  #  CAS authentication successful.
CAS_COOKIE_EXPIRED = 1  #  PYCAS cookie exceeded its lifetime.
CAS_COOKIE_INVALID = 2  #  PYCAS cookie is invalid (probably corrupted).
CAS_TICKET_INVALID = 3  #  CAS server ticket invalid.
CAS_GATEWAY = 4  #  CAS server returned without ticket while in gateway mode.


#  Status codes returned internally by function get_cookie_status().
COOKIE_AUTH = 0  #  PYCAS cookie is valid.
COOKIE_NONE = 1  #  No PYCAS cookie found.
COOKIE_GATEWAY = 2  #  PYCAS gateway cookie found.
COOKIE_INVALID = 3  #  Invalid PYCAS cookie found.

#  Status codes returned internally by function get_ticket_status().
TICKET_OK = 0  #  Valid CAS server ticket found.
TICKET_NONE = 1  #  No CAS server ticket found.
TICKET_INVALID = 2  #  Invalid CAS server ticket found.

CAS_MSG = (
"CAS authentication successful.",
"PYCAS cookie exceeded its lifetime.",
"PYCAS cookie is invalid (probably corrupted).",
"CAS server ticket invalid.",
"CAS server returned without ticket while in gateway mode.",
)

# ##Optional log file for debugging
# ##LOG_FILE="/tmp/cas.log"


#-----------------------------------------------------------------------
#  Imports
#-----------------------------------------------------------------------
import os
import cgi
import md5
import time
import urllib
import urlparse


#-----------------------------------------------------------------------
#  Functions
#-----------------------------------------------------------------------

#  For debugging.
def writelog(msg):
    f = open(LOG_FILE, "a")
    timestr = time.strftime("%Y-%m-%d %H:%M:%S ");
    f.write(timestr + msg + "\n");
    f.close()

#  Used for parsing xml.  Search str for first occurance of
#  <tag>.....</tag> and return text (striped of leading and
#  trailing whitespace) between tags.  Return "" if tag not
#  found.
def parse_tag(str, tag):
   tag1_pos1 = str.find("<" + tag)
   #  No tag found, return empty string.
   if tag1_pos1 == -1: return ""
   tag1_pos2 = str.find(">", tag1_pos1)
   if tag1_pos2 == -1: return ""
   tag2_pos1 = str.find("</" + tag, tag1_pos2)
   if tag2_pos1 == -1: return ""
   return str[tag1_pos2 + 1:tag2_pos1].strip()


#  Split string in exactly two pieces, return '' for missing pieces.
def split2(str, sep):
    parts = str.split(sep, 1) + ["", ""]
    return parts[0], parts[1]

#  Use hash and secret to encrypt string.
def makehash(str, secret=SECRET):
    m = md5.new()
    m.update(str)
    m.update(SECRET)
    return m.hexdigest()[0:8]
    
    
#  Form cookie
def make_pycas_cookie(val, domain, path, secure, expires=None):
    cookie = "Set-Cookie: %s=%s;domain=%s;path=%s" % (PYCAS_NAME, val, domain, path)
    if secure:
        cookie += ";secure"
    if expires:
        cookie += ";expires=" + expires
    return cookie

#  Send redirect to client.  This function does not return, i.e. it teminates this script.
def do_redirect(cas_host, service_url, opt, secure):
    cas_url = cas_host + "/cas/login?service=" + service_url
    if opt in ("renew", "gateway"):
        cas_url += "&%s=true" % opt
    #  Print redirect page to browser
    print "Refresh: 0; url=%s" % cas_url
    print "Content-type: text/html"
    if opt == "gateway":
        domain, path = urlparse.urlparse(service_url)[1:3]
        print make_pycas_cookie("gateway", domain, path, secure)
    print """
If your browser does not redirect you, then please follow <a href="%s">this link</a>.
""" % (cas_url)
    raise SystemExit
    



#  Retrieve id from pycas cookie and test data for validity 
# (to prevent mailicious users from falsely authenticating).
#  Return status and id (id will be empty string if unknown).
def decode_cookie(cookie_vals, lifetime=None):

    #  Test for now cookies
    if cookie_vals == None:
        return COOKIE_NONE, ""

    #  Test each cookie value
    cookie_attrs = []
    for cookie_val in cookie_vals:
        #  Remove trailing ;
        if cookie_val and cookie_val[-1] == ";":
            cookie_val = cookie_val[0:-1]

        #  Test for pycas gateway cookie
        if cookie_val == "gateway":
            cookie_attrs.append(COOKIE_GATEWAY)

        #  Test for valid pycas authentication cookie.
        else:
            # Separate cookie parts
            oldhash = cookie_val[0:8]
            timestr, id = split2(cookie_val[8:], ":")
            #  Verify hash
            newhash = makehash(timestr + ":" + id)
            if oldhash == makehash(timestr + ":" + id):
                #  Check lifetime
                if lifetime:
                    if str(int(time.time() + int(lifetime))) < timestr:
                        #  OK:  Cookie still valid.
                        cookie_attrs.append(COOKIE_AUTH)
                    else:
                        # ERROR:  Cookie exceeded lifetime
                        cookie_attrs.append(COOKIE_EXPIRED)
                else:
                    #  OK:  Cookie valid (it has no lifetime)
                    cookie_attrs.append(COOKIE_AUTH)
                    
            else:
                #  ERROR:  Cookie value are not consistent
                cookie_attrs.append(COOKIE_INVALID)

    #  Return status according to attribute values

    #  Valid authentication cookie takes precedence
    if COOKIE_AUTH in cookie_attrs:
        return COOKIE_AUTH, id
    #  Gateway cookie takes next precedence
    if COOKIE_GATEWAY in cookie_attrs:
        return COOKIE_GATEWAY, ""
    #  If we've gotten here, there should be only one attribute left.
    return cookie_attrs[0], ""


#  Validate ticket using cas 1.0 protocol
def validate_cas_1(cas_host, service_url, ticket):
    #  Second Call to CAS server: Ticket found, verify it.
    cas_validate = cas_host + "/cas/validate?ticket=" + ticket + "&service=" + service_url
    f_validate = urllib.urlopen(cas_validate)
    #  Get first line - should be yes or no
    response = f_validate.readline()
    #  Ticket does not validate, return error
    if response == "no\n":
        f_validate.close()
        return TICKET_INVALID, ""
    #  Ticket validates
    else:
        #  Get id
        id = f_validate.readline()
        f_validate.close()
        id = id.strip()
        return TICKET_OK, id



#  Validate ticket using cas 2.0 protocol
#    The 2.0 protocol allows the use of the mutually exclusive "renew" and "gateway" options.
def validate_cas_2(cas_host, service_url, ticket, opt):
    #  Second Call to CAS server: Ticket found, verify it.
    logging.info(service_url[0:service_url.index('%26ticket')])
    cas_validate = cas_host + "/cas/serviceValidate?ticket=" + ticket + "&service=" + service_url[0:service_url.index('%26ticket')]
#     logging.info("urllib.urlopen(" + cas_validate + ")")
    if opt:
        cas_validate += "&%s=true" % opt
    f_validate = urllib.urlopen(cas_validate)
    #  Get first line - should be yes or no
    response = f_validate.read()
    id = parse_tag(response, "cas:user")
    #  Ticket does not validate, return error
    if id == "":
        return TICKET_INVALID, ""
    #  Ticket validates
    else:
        return TICKET_OK, id


#  Read cookies from env variable HTTP_COOKIE.
def get_cookies():
    #  Read all cookie pairs
    try:
        cookie_pairs = os.getenv("HTTP_COOKIE").split()
    except AttributeError:
        cookie_pairs = []
    cookies = {}
    for cookie_pair in cookie_pairs:
        key, val = split2(cookie_pair.strip(), "=")
        if cookies.has_key(key):
            cookies[key].append(val)
        else:
            cookies[key] = [val, ]
    return cookies


#  Check pycas cookie
def get_cookie_status():
    cookies = get_cookies()
    return decode_cookie(cookies.get(PYCAS_NAME))


def get_ticket_status(cas_host, service_url, protocol, opt):
#     logging.info("get_ticket_status: cas_host: " + cas_host + " service_url: " + service_url + " protocol: " + str(protocol) + " opt: " + opt)
    if cgi.FieldStorage().has_key("ticket"):
        ticket = cgi.FieldStorage()["ticket"].value
#         logging.info("ticket: " + ticket)
        if protocol == 1:
            ticket_status, id = validate_cas_1(cas_host, service_url, ticket, opt)
        else:
#             logging.info("validate_cas_2 using service_url: " + service_url + " and ticket: " + ticket)
            ticket_status, id = validate_cas_2(cas_host, service_url, ticket, opt)
        #  Make cookie and return id
        if ticket_status == TICKET_OK:
            return TICKET_OK, id
        #  Return error status
        else:
            return ticket_status, ""
    else:
        return TICKET_NONE, ""


#-----------------------------------------------------------------------
#  Exported functions
#-----------------------------------------------------------------------

#  Login to cas and return user id.
#
#   Returns status, id, pycas_cookie.
#
def login(cas_host, service_url, lifetime=None, secure=1, protocol=2, path="/", opt=""):

    #  Check cookie for previous pycas state, with is either
    #     COOKIE_AUTH    - client already authenticated by pycas.
    #     COOKIE_GATEWAY - client returning from CAS_SERVER with gateway option set.
    #  Other cookie status are 
    #     COOKIE_NONE    - no cookie found.
    #     COOKIE_INVALID - invalid cookie found.
    cookie_status, id = get_cookie_status()

#     logging.info("cookie_status: " + str(cookie_status))
    if cookie_status == COOKIE_AUTH:
        return CAS_OK, id, ""

    if cookie_status == COOKIE_INVALID:
        return CAS_COOKIE_INVALID, "", ""

    #  Check ticket ticket returned by CAS server, ticket status can be
    #     TICKET_OK      - a valid authentication ticket from CAS server
    #     TICKET_INVALID - an invalid authentication ticket.
    #     TICKET_NONE    - no ticket found.
    #  If ticket is ok, then user has authenticated, return id and 
    #  a pycas cookie for calling program to send to web browser.
#     logging.info("getting ticket status")
    ticket_status, id = get_ticket_status(cas_host, service_url, protocol, opt)

#     logging.info("ticket_status: " + str(ticket_status))
    if ticket_status == TICKET_OK:
        timestr = str(int(time.time()))
        hash = makehash(timestr + ":" + id)
        cookie_val = hash + timestr + ":" + id
        domain = urlparse.urlparse(service_url)[1]
        return CAS_OK, id, make_pycas_cookie(cookie_val, domain, path, secure)

    elif ticket_status == TICKET_INVALID:
        return CAS_TICKET_INVALID, "", ""


    #  If unathenticated and in gateway mode, return gateway status and clear
    #  pycas cookie (which was set to gateway by do_redirect()).
    if opt == "gateway":
        if cookie_status == COOKIE_GATEWAY:
            domain, path = urlparse.urlparse(service_url)[1:3]
            #  Set cookie expiration in the past to clear the cookie.
            past_date = time.strftime("%a, %d-%b-%Y %H:%M:%S %Z", time.localtime(time.time() - 48 * 60 * 60))
            return CAS_GATEWAY, "", make_pycas_cookie("", domain, path, secure, past_date)

    #  Do redirect
    do_redirect(cas_host, service_url, opt, secure)




if __name__ == "__main__":
     app = web.application(urls, globals()).run()
